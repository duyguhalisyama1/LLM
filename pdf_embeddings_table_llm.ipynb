{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1LCmXmpB_xxpM8IC-J50tLC3058IIpsDa",
      "authorship_tag": "ABX9TyOtMsr3Zn34RnzwWnfv4JNz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duyguhalisyama1/LLM/blob/main/pdf_embeddings_table_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ohkfjq5mnbZv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "4457a49c-355c-4b0d-a4ba-fe0f6c51afa9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "GroqError",
          "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the GROQ_API_KEY environment variable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mGroqError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e24386d91f39>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Model ve istemciler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGroq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGROQ_API_KEY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0membedding_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nomic-ai/nomic-embed-text-v2-moe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/groq/_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GROQ_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mapi_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             raise GroqError(\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;34m\"The api_key client option must be set either by passing api_key to the client or by setting the GROQ_API_KEY environment variable\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             )\n",
            "\u001b[0;31mGroqError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the GROQ_API_KEY environment variable"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pinecone import (\n",
        "    Pinecone, ServerlessSpec, CloudProvider, AwsRegion, Metric, VectorType\n",
        ")\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "from groq import Groq\n",
        "import fitz\n",
        "\n",
        "import os\n",
        "import certifi\n",
        "os.environ['SSL_CERT_FILE'] = certifi.where()\n",
        "\n",
        "\n",
        "# API Anahtarlarƒ±\n",
        "load_dotenv()\n",
        "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
        "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
        "\n",
        "# Model ve istemciler\n",
        "client = Groq(api_key=GROQ_API_KEY)\n",
        "embedding_model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v2-moe\", trust_remote_code=True)\n",
        "\n",
        "# üì• PDF'den t√ºm metni √ßek\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with fitz.open(pdf_path) as doc:\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "    return text\n",
        "\n",
        "# üìä Tablo a√ßƒ±klamasƒ± + markdown formatƒ± √ºret\n",
        "def enrich_table_with_context(table_content, document_context):\n",
        "    prompt = f\"\"\"\n",
        "    Given the following table and its context from the original document,\n",
        "    provide a detailed description of the table. Then, include the table in markdown format.\n",
        "\n",
        "    Original Document Context:\n",
        "    {document_context}\n",
        "\n",
        "    Table Content:\n",
        "    {table_content}\n",
        "\n",
        "    Please provide:\n",
        "    1. A comprehensive description of the table.\n",
        "    2. The table in markdown format.\n",
        "    \"\"\"\n",
        "\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that describes tables and formats them in markdown.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "        model=\"llama3-70b-8192\"\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content\n",
        "\n",
        "# üß† Metni embedding'e d√∂n√º≈üt√ºr\n",
        "def get_embedding(text):\n",
        "    return embedding_model.encode(text).tolist()\n",
        "\n",
        "# üß± Pinecone index ba≈ülat\n",
        "def init_pinecone(index_name, dimension=768):\n",
        "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "    indexes = pc.list_indexes()  # Mevcut index'leri alƒ±r\n",
        "\n",
        "    # Eƒüer index mevcutsa, doƒürudan mevcut index'e baƒülan\n",
        "    if index_name in indexes:\n",
        "        print(f\"üîç Var olan Pinecone index'e baƒülanƒ±lƒ±yor: {index_name}\")\n",
        "        return pc.Index(index_name)\n",
        "\n",
        "    try:\n",
        "        # Index yoksa, olu≈üturmayƒ± dene\n",
        "        index_config = pc.create_index(\n",
        "            name=index_name,\n",
        "            dimension=dimension,\n",
        "            metric=Metric.COSINE,  # Vekt√∂r kar≈üƒ±la≈ütƒ±rma metrik olarak COSINE kullanƒ±lƒ±yor\n",
        "            spec=ServerlessSpec(\n",
        "                cloud=CloudProvider.AWS,\n",
        "                region=AwsRegion.US_EAST_1  # ƒ∞htiyacƒ±nƒ±za g√∂re b√∂lgeyi deƒüi≈ütirebilirsiniz\n",
        "            ),\n",
        "            vector_type=VectorType.DENSE\n",
        "        )\n",
        "        print(f\"‚úÖ Yeni Pinecone index olu≈üturuldu: {index_name}\")\n",
        "        return pc.Index(host=index_config.host)\n",
        "\n",
        "    except Exception as e:\n",
        "        # Eƒüer \"ALREADY_EXISTS\" hatasƒ± alƒ±rsak, mevcut index'e baƒülanƒ±yoruz.\n",
        "        if \"ALREADY_EXISTS\" in str(e):\n",
        "            return pc.Index(index_name)\n",
        "        else:\n",
        "            raise e\n",
        "\n",
        "# üì§ Pinecone'a i√ßerikleri g√∂nder\n",
        "def upsert_elements(index, elements):\n",
        "    vectors = []\n",
        "    for i, el in enumerate(elements):\n",
        "        if el.text:\n",
        "            embedding = get_embedding(el.text)\n",
        "            vectors.append((f\"element-{i}\", embedding, {\"content\": el.text}))\n",
        "    index.upsert(vectors=vectors, namespace=\"doc_namespace\")\n",
        "    print(f\"üìå {len(vectors)} i√ßerik Pinecone'a y√ºklendi.\")\n",
        "\n",
        "# üîé Pinecone'dan bilgi al\n",
        "def retrieve(index, query, top_k=3):\n",
        "    query_embedding = get_embedding(query)\n",
        "    res = index.query(\n",
        "        vector=query_embedding,\n",
        "        top_k=top_k,\n",
        "        include_metadata=True,\n",
        "        namespace=\"doc_namespace\"\n",
        "    )\n",
        "    return \"\\n\\n---\\n\\n\".join([match[\"metadata\"][\"content\"] for match in res[\"matches\"]]) if res[\"matches\"] else \"No relevant context found.\"\n",
        "\n",
        "# üß† LLM ile cevap olu≈ütur\n",
        "def complete(prompt):\n",
        "    try:\n",
        "        chat_completion = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "            ],\n",
        "            model=\"llama3-70b-8192\"\n",
        "        )\n",
        "        return chat_completion.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        return f\"Groq API Error: {str(e)}\"\n",
        "\n",
        "# üöÄ Ana S√ºre√ß\n",
        "def main():\n",
        "    pdf_path = \"bav99.pdf\"\n",
        "    excel_file = 'Ek-3 Temel √ñznitelik Tanƒ±mlarƒ±.xlsx'\n",
        "    filter_value = 'Rectifiers'\n",
        "    index_name = \"pdf-embeddings-table-context\"\n",
        "\n",
        "    print(\"üìÑ PDF i√ßeriƒüi i≈üleniyor...\")\n",
        "    elements = partition_pdf(\n",
        "        filename=pdf_path,\n",
        "        strategy=\"hi_res\",\n",
        "        chunking_strategy=\"by_title\"\n",
        "    )\n",
        "    document_context = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    print(\"üîç Tablo √∂ƒüeleri baƒülamsal olarak zenginle≈ütiriliyor...\")\n",
        "    for element in elements:\n",
        "        if element.category == 'Table':\n",
        "            table_content = element.text\n",
        "            enriched_text = enrich_table_with_context(table_content, document_context)\n",
        "            element.text = enriched_text\n",
        "\n",
        "    print(\"üìå Pinecone index ba≈ülatƒ±lƒ±yor...\")\n",
        "    index = init_pinecone(index_name)\n",
        "    index.delete(delete_all=True, namespace=\"doc_namespace\")\n",
        "\n",
        "    print(\"üîó Embedding'ler olu≈üturuluyor ve Pinecone'a g√∂nderiliyor...\")\n",
        "    upsert_elements(index, elements)\n",
        "\n",
        "    # √ñrnek sorgular (Excel'e baƒülƒ± alan tarama)\n",
        "    df = pd.read_excel(excel_file)\n",
        "    fields = df[df.iloc[:, 6] == filter_value].iloc[:, 8].dropna().tolist()\n",
        "\n",
        "    for field in fields:\n",
        "        query_prompt = f\"Find the exact value for: {field} in the datasheet.\"\n",
        "        retrieved_context = retrieve(index, query_prompt)\n",
        "\n",
        "        full_prompt = f\"\"\"\n",
        "        Answer the question based on the context below. Extract ONLY the exact numerical value and unit OR specified term.\n",
        "        If the value is not available in the given context, return \"Not Available\" and nothing else.\n",
        "        Consider synonyms, abbreviations, and variations of the term while searching.\n",
        "\n",
        "        Context:\n",
        "        {retrieved_context}\n",
        "\n",
        "        Question: What is the exact value for {field} (including any synonymous terms or variations)?\n",
        "\n",
        "        Answer:\n",
        "        \"\"\"\n",
        "        answer = complete(full_prompt)\n",
        "        print(f\"{field}: {answer}\")\n",
        "\n",
        "    print(\"üßπ Pinecone temizleniyor...\")\n",
        "    #index.delete(delete_all=True, namespace=\"doc_namespace\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install tools\n",
        "!pip install python-dotenv\n",
        "!apt-get install poppler-utils\n",
        "\n"
      ],
      "metadata": {
        "id": "SX9002yEai7C",
        "outputId": "8d67c90c-485d-4e1e-b042-8b08a8652452",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tools in /usr/local/lib/python3.11/dist-packages (0.1.9)\n",
            "Requirement already satisfied: pytils in /usr/local/lib/python3.11/dist-packages (from tools) (0.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from tools) (1.17.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from tools) (5.3.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.7 [186 kB]\n",
            "Fetched 186 kB in 1s (234 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 126332 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.7_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.7) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.7) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/duyguhalisyama1/LLM.git\n",
        "%cd LLM"
      ],
      "metadata": {
        "id": "AGLZlfk5ICd_",
        "outputId": "47991f91-915d-4c3a-ff5e-15f868ca9f47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLM'...\n",
            "remote: Enumerating objects: 17, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 17 (delta 1), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (17/17), 3.69 MiB | 16.27 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n",
            "/content/LLM\n"
          ]
        }
      ]
    }
  ]
}